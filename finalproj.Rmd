---
title: "'Bombs' in Innistrad: Midnight Hunt"
author: "Cooper Coradeschi, Sadie Cutler, Dane Miller"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{R echo = FALSE}
mid_data <- read.csv("mid.csv")
vow_data <- read.csv("vow.csv")

mid_draft_data <- read.csv("mid_draft.csv")
winrate_draft_data <-read.csv("win_rate.csv")



game_wins = 114
game_losses = 60

#bomb cards that were drafted

vow_bombs = c("Avabruck Caretaker", "Dreadfast Demon", "Toxrill the Corrosive", "Henrika Domnathi", "Cemetary Desecrator", "Halana and Alena Partners", "Sorin the Mirthless", "Manaform Hellkite", "Wedding Annoucement")

mid_bombs = c("The Meathhook Massacre", "Wrenn and Seven", "Tovolar's Huntmaster", "Leisa Forgotten Archangel", "Sigarda Champion of Light",
                "Consuming Blob", "Arlinn the Pack's Hope", "Burn Down the House", "Poppet Stitcher", "Tainted Adversary", "Intrepid Adversary",
                "Brutal Cathar", "Tovolar Dire Overlord", "Sunstreak Phoenix", "Florian Voldaren Scion", "Moonveil Regen", "Augur of Autumn")


```


## Introduction

Magic: The Gathering is a collectible card game with a variety of ways to play. For this analysis, we’ll be focussing on drafts. In a draft, eight players sit around a table, each with three packs of fifteen cards. All eight players open the first pack, pick a single card from it, and then pass the remaining cards to their left. This is repeated until the first pack is empty, and then the second is opened. This time players pass to the right. Finally, the third pack is opened and passed again to the left. This process leaves each player with forty five hand picked cards in their pool, typically spanning only two of the five colors. From these cards, each player makes a forty card minimum deck of twenty three of those drafted cards and seventeen lands. Once the deck building process is complete, the eight decks are played against each other to determine the best of the pod.

What we are most interested in is the contents of each pack. A draft booster pack contains fifteen cards, but these cards are not created equally. There is one land card, ten common cards, and three uncommon cards. The last card in the pack is either a rare or a mythic rare card. While there are many exceptions to the rule, the mythic rare cards are typically the best, then the rares, the uncommons, and finally the commons. However, since the distribution is so heavily weighted towards the lower rarities, most games are decided by the quality and synergy of a deck's common cards.

Roughly every three months, a brand new Magic: The Gathering set is released which typically contains 350 cards. These are the cards that players will find in their draft booster packs when drafting the set. The primary focus of our study deals with the unique environments of each set. One particular characteristic of a set is whether it is a ‘prince’ or ‘pauper’ set. A ‘prince’ set is a set where the rare and mythic rare cards outperform the commons and uncommons by so much that games are almost exclusively decided by who plays an incredible rare first. These cards, which can single handedly determine the outcome of games, are called ‘bomb’. A ‘pauper’ set is the opposite, where even incredible bombs struggle to keep up with good synergies. In these sets, games are determined by several cards working together at once to generate value, and the most cohesive decks tend to win.

There are a lot of factors that go into making a set fun, and the best sets get drafted frequently for all three months before a new set comes out. Other sets see the player base dwindle quickly, and even dedicated drafters are desperate for a new set by the three month mark. Balancing the power of rares and mythic rares is often a key part of this. 

The purpose of this study is to determine the significance of 'bomb' cards in the Midnight Hunt set. The Midnight Hunt set is referred to as a 'pauper set', so we expect that good value and synergistic commons and uncommons have more significant impact on games than 'bombs'. To determine this, we first conducted our own study of drafts within the set, and then also compared it to larger data sets including other Magic: The Gathering sets around the same time.

 <br> <br> <br>



```{R echo = FALSE}

#hist(neo_data$OH.WR, freq = F, col=rgb(0,0,1,alpha=0.2))
#hist(mid_data$OH.WR, freq = F, col=rgb(0,1,0,alpha=0.2), add = T)
#hist(vow_data$OH.WR, freq = F, col=rgb(1,0,0,alpha=0.2), add = T)
```

```{R echo = FALSE}
#c(neo_data$Name[which.max(neo_data$GP.WR)], mid_data$Name[which.max(mid_data$GP.WR)], vow_data$Name[which.max(vow_data$GP.WR)])
```

## Data Collection and Limitations
To dive deep into the set, we drafted 25 times, resulting in 174 games played, and tracked which cards ended up in our final decks. Since we draft forty five cards each time but only typically play twenty three of them, we made sure to not include cards that sat around doing nothing. Unfortunately, while 25 drafts let us play with all of the important commons and uncommons of the set, it did not allow for every rare and mythic rare to be seen and played. Additionally, some cards were seen in draft but never considered good enough to be picked, and some picked cards were never considered good enough to be played. This means that our data set will not include some cards at the very top of the spectrum (excellent cards that are rarely opened and never passed) and some cards at the very bottom of the spectrum (cards we refused to pick or play). While ideally our data set contains every card several times, 25 drafts is certainly enough games to get a general feel for the format and analyze the bombs that we did have a chance to play.

Another limitation of our data is that color pairs matter in Magic: The Gathering. In Midnight Hunt specifically, blue and black cards over performed, while red and green cards substantially under performed, leaving white somewhere in the middle. Midnight Hunt is a two color set, meaning that all ten two color combinations of those five colors make up the deck archetypes. Of course, some players blur the lines and 'splash' a third (or even more) colors, but the decks are primary only two colors. This matters in a few cases - let's consider the cards "Arlinn, the Pack's Hope" and "Tovolar, Dire Overlord". Arlinn and Tovolar are absolute bombs on paper and by most metrics. According to 17lands, a huge data set we pulled from to compare with our study, drawing either of those cards in a game increase your odds of winning by just over 6.5%, which is a huge impact for a singe card. However, both of these cards fall beneath the data set average win rate when put into a deck by almost 3%. This is where color pairing shows its significance. Both of these cards require both red and green mana, which means they are almost exclusively played in by far the weakest color pairing in the format. Compare that to "Drownyard Amalgam", a clunky blue common that plays best in the blue and black deck. While it sees an exact 0% increase in chance to win when played, it has a just over 2% higher overall win rate than Arlinn or Tovolar. Despite being a much less impactful card, it sees substantially better win rates because it is surrounded by higher quality cards on average in those colors. The data we collected does not take color pairings into account. Because there are ten different color pairings, separating 25 drafts into 2.5 drafts per deck would make for a very sparse data set. By combining them, we can display all 25 drafts for a total of 174 games, which will give us much more accurate win rates and more data points for a line of best fit.

## MID Draft Analysis
Below is a plot of all the cards we played plotted on wins and losses. The line representing our average win rate shows that cards above the line over performed and that cards below the line under performed.
```{R echo = FALSE}

total_played = sum(mid_draft_data$Wins) + sum(mid_draft_data$Losses)
num_wins = sum(mid_draft_data$Wins)
num_loss = sum(sum(mid_draft_data$Losses))

plot(x = mid_draft_data$Losses, y = mid_draft_data$Wins, col=ifelse(mid_draft_data$Card %in% mid_bombs, 'red', 'grey'), type = "p")  #plot the wins over the losses
abline(a = 0, b = game_wins/game_losses)                              #add a line with the slope of our wins over losses to see how are data                                                                          #compare
lim = c(0,20)

plot(mid_draft_data$Losses, mid_draft_data$Wins, xlim = lim, ylim = lim, col=ifelse(mid_draft_data$Card %in% mid_bombs, 'red', 'grey'), type = 'p')
abline(a = 0, b = sum(mid_draft_data$Wins, na.rm = T)/sum(mid_draft_data$Losses, na.rm = T))
```
<br>Here we’ve highlighted the ‘bomb’s of the set. Each red dot represents an exceptionally powerful card. As you can see, these cards didn’t overperform in our decks. As a matter of fact, most of them underperformed. We had the opportunity to play with eight of the seventeen cards regarded as ‘bomb’ (see https://draftsim.com/MID-pick-order.php), and five of those cards actually had lower win rates than our average. Only three cards, Consuming Blob, Tovolar’s Huntmaster, and Liesa, Forgotten Angel actually outperformed our average win rate, and even then they were themselves outperformed by some common cards in the set. Consider Celestus Sanctifier, an underwhelming common with decent stats and minimal upside. Not only was this one of our most played cards, it also boasted a colossal 82.5% win rate over fifty seven games played. Obviously, it’s a worse card than Liesa, Forgotten Angel by itself, but it is easy to cast and helps synergize excellently with other cards like Mourning Patrol. 


## VOW Data Set Analysis
```{R echo = FALSE}

vow_win = c(vow_data$GP.WR/100 * vow_data$X..GP)
vow_loss = c(vow_data$X..GP - vow_win)

plot(vow_loss, vow_win, col=ifelse(vow_data$Name %in% vow_bombs, 'red', 'grey'), type = 'p')
abline(a = 0, b = sum(vow_win, na.rm = T)/sum(vow_loss, na.rm = T))

lim = c(0,20000)

plot(vow_loss, vow_win, xlim = lim, ylim = lim, col=ifelse(vow_data$Name %in% vow_bombs, 'red', 'grey'), type = 'p')
abline(a = 0, b = sum(vow_win, na.rm = T)/sum(vow_loss, na.rm = T))
```
<br>This compares the wins and losses of draft games from the VOW magic set. The red points represent bombs as determined from IWD rates (IWD stands for Improvement When Drawn) and analysis regarding the overall impact the cards have in a game. From this graph, the red points are all above the base win rate calculated from the entire set, showing that these cards would routinely improve player games when used.


## MID Data Set Analysis
```{R echo = FALSE}

mid_win = c(mid_data$GP.WR/100 * mid_data$X..GP)
mid_loss = c(mid_data$X..GP - mid_win)

plot(mid_loss, mid_win, col=ifelse(mid_data$Name %in% mid_bombs, 'red', 'grey'), type = 'p')
abline(a = 0, b = sum(mid_win, na.rm = T)/sum(mid_loss, na.rm = T))

lim = c(0,15000)

plot(mid_loss, mid_win, xlim = lim, ylim = lim, col=ifelse(mid_data$Name %in% mid_bombs, 'red', 'grey'), type = 'p')
abline(a = 0, b = sum(mid_win, na.rm = T)/sum(mid_loss, na.rm = T))

#mid_data$Name[which.min(mid_bombs %in% mid_data$Name)] # trying to get lowest performing bombs
```
<br>In the mid data set we see that while there are slightly more bombs above the win/loss line, the dispersion is pretty even and in addition none of the bombs are actually very far from the line. Instead the cards that vary the most from the typical win/loss line are non-bombs, this shows that in general the bombs are less likely to decide the outcome of a game when drawn than other, more common cards. 
You can also see that the bombs are typically grouped towards the bottom of the graph, this is simply because they're less common and therefore have fewer wins/losses. To give you a better view of the bombs we cut out the upper ranges of the total mid data set to zoom more onto the bombs themselves.

## MID and VOW comparative card analysis
The improvement when drawn statistic is very important in this game because it shows how much a card impacts a players hand during a game. A negative improvement when drawn means that the card decreases a player's current status in the game, whereas a positive IWD increases a player's chance of winning. A density line will be plotted to help determine the distribution in each set.
```{R echo = FALSE}
mid_density <- mid_data$IWD
mid_density <- na.omit(mid_density)

hist(mid_data$IWD, breaks = 10, col = "orange", main = "Histogram of MID Improvement When Drawn with Density plot", xlim = c(-10,20), ylim = c(0,0.12), xlab = "Improvement When Drawn", prob = TRUE)
lines(density(mid_density), col = 4, lwd = 2)

vow_density <- vow_data$IWD
vow_density <- na.omit(vow_density)

hist(vow_data$IWD, breaks = 10, col = "orange", main = "Histogram of VOW Improvement When Drawn with Density plot", xlim = c(-10,20), ylim = c(0,0.12), xlab = "Improvement When Drawn", prob = TRUE)
lines(density(vow_density), col = 4, lwd = 2)
```
<br> The MID data set is centered around 0, meaning that many of the cards don't have much significance in game meaning they are not 'bombs' and dramatically alter your current standing in the game. There is a slight increase in density around 5% IWD, showing that many cards, while most likely commons, do generally improve a player's hand. This data set is not significantly skewed indicating that MID is a 'pauper' set where there are not many 'bombs' in the set. On the other hand, the VOW histogram is skewed to the right. This means that many cards had a positive improvement in a  deck and that there are outliers in the set. An interesting thing to note is that the commons (where the higher density is between -5 and 2.5) are skewed slightly to the left, meaning that many common cards either had no impact or a negative impact on the game. This increases the importance of those 'bomb' cards to win a game. The difference between the VOW and MID histogram further support that MID is a 'pauper' set because it not nearly as skewed as the VOW data set and has a clear emphasis in common cards compared to the VOW set.

## Analysis of MID and VOW outliers
```{R echo = FALSE}
vow_boxplot <- boxplot(vow_data$IWD, main = "VOW Data Boxplot", ylab = "Improvement When Drawn")

vow_outliers <- vow_data[vow_data$IWD %in% vow_boxplot$out,]

vow_outliers_names <- vow_outliers$Name
vow_outliers_names

number_of_vow_outliers <- length(vow_outliers_names)
number_of_vow_outliers

number_of_vow_cards <- length(vow_data$Name)

percent_of_vow_outliers <- number_of_vow_outliers / number_of_vow_cards
percent_of_vow_outliers
```
<br>Here you can see that there are 14 outliers in the VOW set, making 5% of the cards outliers that would significantly affect your win rate when drawn. When your deck is only 40 cards, it means you should end up with 2 cards in your deck that could change the direction of your game significantly, while that may not seem like a lot, in comparison with the mid data you'll see below it's evidently much more volatile.


```{R echo = FALSE}
mid_boxplot <- boxplot(mid_data$IWD, main = "MID Data Boxplot", ylab = "Improvement When Drawn")

mid_outliers <- mid_data[mid_data$IWD %in% mid_boxplot$out,]

mid_outliers_names <- mid_outliers$Name
mid_outliers_names

number_of_mid_outliers <- length(mid_outliers_names)
number_of_mid_outliers

number_of_mid_cards <- length(mid_data$Name)

percent_of_mid_outliers <- number_of_mid_outliers / number_of_mid_cards
percent_of_mid_outliers

```

<br>In the MID box plot there are only 2 outliers, making outliers 0.7% of the data set. This means that the mid set is 7 times *less likely* than the vow set to produce a card that could significantly change the outcome of the game. As a result games using the vow set can be less skill based and more luck based than mid. This diversion from skill into luck is what makes a deck less "pauperish" and what makes mid so well balanced.
These box plots highlight the significance of "bomb" cards in the VOW magic set compared to a more balanced magic such as the MID set. The Improvement when drawn is a statistics that removes all the times that a player has the card in their deck but is not seen during the game. This data only shows how the card directly impacts the game. As shown, VOW has signifcantly more cards that could greatly impact (whether positively or negatively) a players game when drawn.


## Hypothesis Test:
```{R echo = FALSE}
mid_draft_win_rate <- c()

for(i in mid_draft_data) {
  mid_draft_win_rate <- ((mid_draft_data$Wins / (mid_draft_data$Wins + mid_draft_data$Losses))) * 100
}

t.test(x = mid_data$GP.WR, y = mid_draft_win_rate, conf.level = .995)

```
<br>A Welch t-test was used to compare the large data set from 17lands.com and the gathered draft data obtained for this assignment. The p-value is significantly below 0.05, so we can reject the null hypothesis showing that our data fits with the very large user base data that was obtained from 17lands.com. From the 25 drafts records for the drafted data set, the results were very similar to the large data set obtained from 17lands.com. 


## Conclusion


The data set gathered from the 25 drafts were very statistically significant compared to the much larger MID data set obtained from 17lands.com. The data gathered is representative of the average user experience. Midnight Hunt had fewer outliers in the set compared to other recent sets. Individual cards in the VOW set were much more likely to determine the outcome of a game. These 'bombs' in Crimson Vow were largely positive, meaning that if a player drew one of the 'bomb' cards, the likelihood of winning dramatically improved. If this experiment were to be repeated, a key point of data to collect would be color pairings and the strengths and weaknesses of each color within the formats. More data would need to be taken, but this would help eliminate any bias that could occur when dealing with 'bombs' in poor color pairs. This refers back to the problem with Arlinn and Tovolar, but extends into other sets and color pairs as well to varying degrees.